# Bayesian AB Testing references
# https://cran.r-project.org/web/packages/bayesAB/bayesAB.pdf
# https://cran.r-project.org/web/packages/bayesAB/vignettes/introduction.html
# https://github.com/FrankPortman/bayesAB
# https://www.machinegurning.com/rstats/bayes_r/

# prior probabilities
# https://en.wikipedia.org/wiki/Prior_probability

# good explanation of Bernoulli as special case of Binomial distribution
# https://math.stackexchange.com/questions/838107/what-is-the-difference-and-relationship-between-the-binomial-and-bernoulli-distr

# another neat article on bayesian methods for testing hypothesis
# http://varianceexplained.org/r/bayesian_ab_baseball/

# Some readings on selecting Beta params, alpha and beta
# https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution
# https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors
# https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing

# p-value hacking
# http://varianceexplained.org/r/bayesian-ab-testing/

# 
# https://www.r-bloggers.com/bayesab-0-7-0-a-primer-on-priors/

# great walkthtough using python
# https://towardsdatascience.com/bayesian-a-b-testing-with-python-the-easy-guide-d638f89e0b8a

library(tidyverse)
library(bayesAB)

# # Ideally, we would conduct a power test to determine the sample size required to detect a small differences (h=0.1)
# # in conversion rate with 5% Type I error rate (sig.level=0.05) and 20% Type II error (power=1-TypeII=0.80).
# library(pwr)
# power_test <- pwr.2p.test(h=0.1, 
#                           sig.level=0.05, 
#                           power=0.8)
# # Grab the sample size generated by our power test.
# n <- ceiling(power_test$n)
# n

data <- read.csv('ab_data_session_grain.csv')
# View(data)

pretest_A <- data %>% filter(test_flag=='pre_test' & webflow_id=='A')
# View(pretest_A)

test_A <- data %>% filter(test_flag=='test' & webflow_id=='A')
# View(test_A)

test_B <- data %>% filter(test_flag=='test' & webflow_id=='B')
# View(test_B)

# function to obtain some stats to be used in the hypothesis test
get_stats <- function(x) {
    conversions <-sum(x)
    sample_size <- length(x)
    conversion_rate <- round(conversions/sample_size, 4)
    variance <- conversion_rate*(1-conversion_rate)/sample_size
    lower_95 <- round(conversion_rate - 1.96*sqrt(variance), 4)
    upper_95 <- round(conversion_rate + 1.96*sqrt(variance), 4)
    my_stats <- list(conversions = conversions,
                     sample_size = sample_size,
                     conversion_rate = conversion_rate, 
                     variance = variance,
                     lower_95 = lower_95,
                     upper_95 = upper_95)
    return(my_stats)
}

pretest_A_stats <- get_stats(x=pretest_A$is_conversion)
pretest_A_stats

test_A_stats <- get_stats(x=test_A$is_conversion)
test_A_stats

test_B_stats <- get_stats(x=test_B$is_conversion)
test_B_stats

# hypothesis test 1... H0: pretest A conversion rate = test A conversion rate 
# p > 0.05, so we fail to reject the null hypothesis 
prop.test(x = c(pretest_A_stats$conversions, test_A_stats$conversions), 
          n = c(pretest_A_stats$sample_size, test_A_stats$sample_size),
          conf.level = 0.95,
          alternative = 'two.sided')

# hypothesis test 2... H0: test A conversion rate = test B conversion rate 
# p < 0.05, so we reject the null hypothesis 
prop.test(x = c(test_A_stats$conversions, test_B_stats$conversions), 
          n = c(test_A_stats$sample_size, test_B_stats$sample_size),
          conf.level = 0.95,
          alternative = 'two.sided')

# Instead of relying on a traditional hypothesis test...
#
# If we think about is_conversion as a vector of Bernoulli RV's, then we can use the beta distribution as the conjugate prior of p.  
#
# For clarity, we can imagine is_order as having been generated as follows,
#   is_conversion <- rbinom(n,1,p) 
# which implies that each,
#   is_conversion[i] ~ Bernoulli(p), 0 <= i <= n
#
# So, we need to find the (hyper)params that appropriately model p ~ Beta(alpha, beta).  To do so, we will 
# use our historical knowledge about p from the pretest A data. Specifically, we calculated the sample conversion 
# rate, aka the sample proportion p_hat, which estimates p, the population proportion, or true conversion probability.  
# 
#   p_hat = conversions / (conversions + drop_offs)
#         = 627 / (627 + 4373)
#         = 0.1254 
#
#   95% CI: (0.1162, 0.1346)
#
# We also know the mean of the Beta distribution is given by,
#
#   E(p) = alpha / (alpha + beta)
#
# which looks very much like our defintion of p_hat.  So, we want p ~ Beta(alpha=627, beta=4373).
#
# We'll add the point estimate p_hat as well as the bounds of 95% CI for reference.

# Scroll down to Details > Bernoulli
?bayesTest

prior_alpha <- pretest_A_stats$conversions
prior_beta <- pretest_A_stats$sample_size - pretest_A_stats$conversions

plot_beta <- plotBeta(alpha=prior_alpha, beta=prior_beta)
plot_beta + 
    geom_vline(xintercept=pretest_A_stats$lower_95, linetype='dashed', color = 'darkblue') +
    geom_vline(xintercept=pretest_A_stats$upper_95, linetype='dashed', color = 'darkblue') +
    geom_vline(xintercept=pretest_A_stats$conversion_rate, linetype='solid', color = 'darkred') +
    xlim(0.10, 0.15) +
    ggtitle(substitute(paste('Conjugate Prior:  p ~ Beta(', alpha, '=', prior_alpha, ', ', beta, '=', prior_beta, ')'), list(prior_alpha=prior_alpha, prior_beta=prior_beta))) +
    theme(plot.title = element_text(hjust = 0.5))

set.seed(444)
# This is our AB Test object    
abTest <- bayesTest(test_A$is_conversion, 
                    test_B$is_conversion, 
                    priors = c('alpha' = prior_alpha, 'beta' = prior_beta), 
                    n_samples = 1e5, 
                    distribution = 'bernoulli')

# Print yields summary statistics of the input data for conversion rates of pages A and B,
# the parameters of the prior, and the number of posterior samples to draw (1e5 is a good rule 
# of thumb and should be large enough for the distribution to converge)
print(abTest)

# Useful test results ... note, these numbers may change slightly each time run bayesTest() object unless you use the set.seed() at line 137.
#
# We are 4.69% certain that flow A is better than flow B, which implies B is better than A.
#
# In addition, we assert that the conversion rate for A is between -11.5% and -0.18% better (bigger) relative to B, 
# again, implying B is defintiely better than A.
#
# The "posterior expected loss for choosing B over A" is 6.5%, which can be interpretted as follows...
# "Based on the current winner, what is the expected loss you would see should you choose wrongly?"
# Or, the P{B>A} * E{(B-A)/A | B>A} ... the mean percentLift of B over A when B>A times the likelihood B>A
# So, given that B is our current winner, we would expect to lose 6.5% in lift (think (B-A)/A percent life calculation)
# if we choose A (the loser) over B (the winner).
summary(abTest)
# percentLift=rep(0, length(abTest$posteriors)),
# credInt=rep(.9, length(abTest$posteriors)))

# Some vizualizations
plot(abTest)


# MonteCarlo simulation to replicate the results of our bayesTest() ... not exact, but very very close
# This should give you an idea as to what is happening behind the scenes
n_trials <- 1e5
a_samples <- rbeta(n_trials,
                   test_A_stats$conversions+prior_alpha,
                   (test_A_stats$sample_size-test_A_stats$conversions)+prior_beta)
b_samples <- rbeta(n_trials,
                   test_B_stats$conversions+prior_alpha,
                   (test_B_stats$sample_size-test_B_stats$conversions)+prior_beta)

# P(A > B)
prob_a_superior <- sum(a_samples > b_samples)/n_trials
prob_a_superior

# Credible Interval on (A - B) / B 
credible_interval <- quantile((a_samples-b_samples)/b_samples, c(.05,.95))
credible_interval

#  Point estimate related to credible interval of A's lift over B ... not included in summary of bayesTest()
a_minus_b_over_b = (mean(a_samples)-mean(b_samples))/mean(b_samples)
a_minus_b_over_b

# Posterior Expected Loss for choosing B over A
BoverA <- b_samples > a_samples
loss <- (b_samples-a_samples)/a_samples
PEL <- mean(BoverA) * mean(loss[BoverA])
PEL





